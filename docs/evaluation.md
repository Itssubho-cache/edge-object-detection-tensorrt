## Evaluation Metrics

The system was evaluated using the following metrics:

- End-to-end latency
- Throughput (Frames Per Second)
- GPU memory utilization

Experiments demonstrate up to a 3Ã— reduction in inference
latency using TensorRT optimization compared to baseline
deployment approaches.
